{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from es_utils import *\n",
    "import math\n",
    "import numpy as np\n",
    "from pq_net import NNetWrapper as nn\n",
    "from pq_net import PQAC\n",
    "from penquest import *\n",
    "from utils import *\n",
    "import os\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[1, 1, 0],\n",
    "         [1, 1, 1],\n",
    "         [0, 1, 1]]\n",
    "\n",
    "m_atts = [[1, 0, 0.5, 0, 0],\n",
    "          [0, 0, 0.5, 0, 0],\n",
    "          [0, 1, 0.5, 0, 0]]\n",
    "\n",
    "p1_atts = [1, 1, 1, 20, 1]\n",
    "p2_atts = [1, 1, 1, 20, 1]\n",
    "\n",
    "pq = PenQuest(weights, m_atts, p1_atts, p2_atts)\n",
    "board, player = pq.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_inputs = len(pq.get_attack_vector(board))\n",
    "defender_inputs = len(pq.get_defend_vector(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['a2c_att_pq_3600.pth.tar',                'a2c_def_pq_3600.pth.tar',\n",
    "             'az_es_att_checkpoint.pth.tar',           'az_es_def_checkpoint.pth.tar',\n",
    "             'az_grad_att_checkpoint.pth.tar',         'az_grad_def_checkpoint.pth.tar',\n",
    "             'cem_att_pq_3600.pth.tar',                'cem_def_pq_3600.pth.tar',\n",
    "             'oneone_att_pq_3600.pth.tar',             'oneone_def_pq_3600.pth.tar']\n",
    "\n",
    "\n",
    "names = ['a2c', 'az_es', 'az_grad', 'cem', 'oneone', 'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAttackAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * self.g.size), 0\n",
    "    \n",
    "class RandomDefenseAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * (self.g.size*2)), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'checkpoint'\n",
    "\n",
    "att_a2c = PQAC(attacker_inputs, pq.size, 64)\n",
    "att_az_es = PQAC(attacker_inputs, pq.size, 64)\n",
    "att_az_grad = PQAC(attacker_inputs, pq.size, 64)\n",
    "att_cem = PQAC(attacker_inputs, pq.size, 64)\n",
    "att_oneone = PQAC(attacker_inputs, pq.size, 64)\n",
    "\n",
    "\n",
    "direct = os.path.join(folder, filenames[0])\n",
    "att_a2c_check = torch.load(direct)\n",
    "att_a2c.load_state_dict(att_a2c_check)\n",
    "\n",
    "direct = os.path.join(folder, filenames[2])\n",
    "att_az_es_check = torch.load(direct)\n",
    "att_az_es.load_state_dict(att_az_es_check['state_dict'])\n",
    "\n",
    "direct = os.path.join(folder, filenames[4])\n",
    "att_az_grad_check = torch.load(direct)\n",
    "att_az_grad.load_state_dict(att_az_grad_check['state_dict'])\n",
    "\n",
    "direct = os.path.join(folder, filenames[6])\n",
    "att_cem_check = torch.load(direct)\n",
    "att_cem.load_state_dict(att_cem_check)\n",
    "\n",
    "direct = os.path.join(folder, filenames[8])\n",
    "att_oneone_check = torch.load(direct)\n",
    "att_oneone.load_state_dict(att_oneone_check)\n",
    "\n",
    "attackers = [att_a2c, att_az_es, att_az_grad, att_cem, att_oneone, RandomAttackAgent(pq)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_a2c = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "def_az_es = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "def_az_grad = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "def_cem = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "def_oneone = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "\n",
    "\n",
    "direct = os.path.join(folder, filenames[1])\n",
    "def_a2c_check = torch.load(direct)\n",
    "def_a2c.load_state_dict(def_a2c_check)\n",
    "\n",
    "direct = os.path.join(folder, filenames[3])\n",
    "def_az_es_check = torch.load(direct)\n",
    "def_az_es.load_state_dict(def_az_es_check['state_dict'])\n",
    "\n",
    "direct = os.path.join(folder, filenames[5])\n",
    "def_a2c_check = torch.load(direct)\n",
    "def_a2c.load_state_dict(def_a2c_check['state_dict'])\n",
    "\n",
    "direct = os.path.join(folder, filenames[7])\n",
    "def_cem_check = torch.load(direct)\n",
    "def_cem.load_state_dict(def_cem_check)\n",
    "\n",
    "direct = os.path.join(folder, filenames[9])\n",
    "def_oneone_check = torch.load(direct)\n",
    "def_oneone.load_state_dict(def_oneone_check)\n",
    "\n",
    "defenders = [def_a2c, def_az_es, def_az_grad, def_cem, def_oneone, RandomDefenseAgent(pq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent_1, agent_2, render = False, temp = 1):\n",
    "    board, player = pq.getInitBoard()\n",
    "    boards = []\n",
    "    players = []\n",
    "    pis = []\n",
    "    values = []\n",
    "    while pq.getGameEnded(board, player) == 0:\n",
    "        valids = pq.getValidMoves(board, player)\n",
    "        if player == 1:\n",
    "            vec = pq.get_attack_vector(board)\n",
    "            probs, value = agent_1.forward(vec)\n",
    "            valids = valids[:pq.size]\n",
    "        elif player == -1:\n",
    "            vec = pq.get_defend_vector(board)\n",
    "            probs, value = agent_2.forward(vec)\n",
    "            valids = valids[pq.size:]\n",
    "        else:\n",
    "            print('bruh')\n",
    "            \n",
    "        probs = probs.detach().numpy()\n",
    "\n",
    "        \n",
    "        boards.append(board)\n",
    "        players.append(player)\n",
    "        pis.append(probs)\n",
    "        values.append(value)\n",
    "        \n",
    "        if temp != 0:\n",
    "            probs = np.power(probs, temp)\n",
    "        probs = np.array(probs) * np.array(valids)\n",
    "        probs = np.squeeze(probs)\n",
    "        if sum(probs) == 0:\n",
    "            print('bruh')\n",
    "        probs = probs / np.sum(probs)\n",
    "        if temp == 0:\n",
    "            action = np.argmax(probs)\n",
    "        else:\n",
    "            action = np.random.choice(len(probs), p = probs)\n",
    "        if player == -1:\n",
    "            action += pq.size\n",
    "        \n",
    "        board, player = pq.getNextState(board, player, action, render = render)\n",
    "        if render:\n",
    "            pq.render(board, player)\n",
    "          \n",
    "    \n",
    "    return pq.getGameEnded(board, player)\n",
    "\n",
    "\n",
    "def attacker_vs_defender(attacker, defender, episode_count, temp = 1):\n",
    "    wins = 0\n",
    "    for k in range(episode_count):\n",
    "        winner = play_game(attacker, defender, temp = temp)\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "            \n",
    "    return wins\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2c\n",
      "[60, 61, 43, 17, 79, 63]\n",
      "az_es\n",
      "[25, 17, 32, 28, 24, 18]\n",
      "az_grad\n",
      "[24, 27, 38, 18, 33, 23]\n",
      "cem\n",
      "[32, 28, 31, 19, 33, 29]\n",
      "oneone\n",
      "[30, 41, 27, 25, 39, 41]\n",
      "random\n",
      "[21, 24, 32, 25, 35, 31]\n"
     ]
    }
   ],
   "source": [
    "wins = []\n",
    "count = 0\n",
    "np.random.seed(1)\n",
    "random.seed(0)\n",
    "for attacker in attackers:\n",
    "    print(names[count])\n",
    "    count += 1\n",
    "    att_wins = []\n",
    "    for defender in defenders:\n",
    "        att_wins.append(attacker_vs_defender(attacker, defender, 100, temp = 1))\n",
    "    wins.append(att_wins)\n",
    "    print(att_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from es_utils import *\n",
    "import math\n",
    "import numpy as np\n",
    "from pq_net import NNetWrapper as nn\n",
    "from pq_net import PQAC\n",
    "from penquest import *\n",
    "from utils import *\n",
    "\n",
    "args = dotdict({\n",
    "    'numIters': 2,\n",
    "    'numEps': 5,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,        #\n",
    "    'updateThreshold': 0.5,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,    # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 5,          # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,         # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'batch_size' : 20,\n",
    "    'elite_size' : 10,\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "})\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # stores #times edge s,a was visited\n",
    "        self.Ns = {}  # stores #times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp == 0:\n",
    "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
    "            bestA = np.random.choice(bestAs)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            return probs\n",
    "\n",
    "        counts = [x ** (1. / temp) for x in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x / counts_sum for x in counts]\n",
    "        return probs\n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        board, player = canonicalBoard\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(board, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # terminal node\n",
    "            return -self.Es[s] * player\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(*canonicalBoard)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return -v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                            1 + self.Nsa[(s, a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(board, player, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = [[1, 1, 0],\n",
    "         [1, 1, 1],\n",
    "         [0, 1, 1]]\n",
    "\n",
    "m_atts = [[1, 0, 0.5, 0, 0],\n",
    "          [0, 0, 0.5, 0, 0],\n",
    "          [0, 1, 0.5, 0, 0]]\n",
    "\n",
    "p1_atts = [1, 1, 1, 20, 1]\n",
    "p2_atts = [1, 1, 1, 20, 1]\n",
    "\n",
    "pq = PenQuest(weights, m_atts, p1_atts, p2_atts)\n",
    "board, player = pq.getInitBoard()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nnet = nn(pq)\n",
    "mcts = MCTS(pq, nnet, args)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "class AlphaZero():\n",
    "    def __init__(self, game, nnet, mcts, args):\n",
    "        self.g = game\n",
    "        self.args = args\n",
    "        self.nnet = nnet\n",
    "        self.mcts = mcts\n",
    "        self.sigma = copy.copy(nnet)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in self.sigma.att_nnet.parameters():\n",
    "                param.divide_(10)\n",
    "            for param in self.sigma.def_nnet.parameters():\n",
    "                param.divide_(10)\n",
    "        \n",
    "    def execute_episode(self, render = False):\n",
    "        board, player = self.g.getInitBoard()\n",
    "        trainExamples = []\n",
    "        att_boards = []\n",
    "        att_pis = []\n",
    "        \n",
    "        def_boards = []\n",
    "        def_pis = []\n",
    "        \n",
    "        total_moves = 0\n",
    "        while True:\n",
    "            canonicalBoard = self.g.getCanonicalForm(board, player)\n",
    "            temp = 1 if total_moves < self.args.tempThreshold else 0\n",
    "            pi = self.mcts.getActionProb(canonicalBoard, temp = temp)\n",
    "            a = np.random.choice(len(pi), p = pi)\n",
    "            \n",
    "            if player == 1:\n",
    "                att_boards.append(board)\n",
    "                att_pis.append(pi)\n",
    "                \n",
    "            elif player == -1:\n",
    "                def_boards.append(board)\n",
    "                def_pis.append(pi)\n",
    "                \n",
    "            else:\n",
    "                print(\"bruhhhh\")\n",
    "\n",
    "                \n",
    "            board, player = self.g.getNextState(board, player, a) \n",
    "            total_moves +=1\n",
    "            \n",
    "            if render:\n",
    "                print('Probs: {}\\nAction: {}\\n'.format(pi, a))\n",
    "                \n",
    "            r = self.g.getGameEnded(board, player)\n",
    "            if r != 0:\n",
    "                return att_boards, att_pis, def_boards, def_pis, r\n",
    "            \n",
    "    def att_pi_clip(self, pi):\n",
    "        return pi[:self.g.size]\n",
    "    \n",
    "    def def_pi_clip(self, pi):\n",
    "        return pi[self.g.size:]\n",
    "    \n",
    "    def train_grad(self):\n",
    "        start = time.time()\n",
    "        att_data = []\n",
    "        def_data = []\n",
    "        for k in range(self.args.numEps):\n",
    "            att_boards, att_pis, def_boards, def_pis, r = self.execute_episode()\n",
    "            att_data += [(pq.get_attack_vector(board), self.att_pi_clip(pi), r) for (board, pi) in zip(att_boards, att_pis)]\n",
    "            def_data += [(pq.get_defend_vector(board), self.def_pi_clip(pi), r) for (board, pi) in zip(def_boards, def_pis)]\n",
    "            \n",
    "        board, player = self.g.getInitBoard()\n",
    "        att_boards, att_pis, att_vs = list(zip(*att_data))\n",
    "        \n",
    "        att_boards = np.array(list(att_boards))\n",
    "        att_pis = torch.FloatTensor(np.array(list(att_pis)))\n",
    "        att_vs = torch.FloatTensor(np.array(list(att_vs)))\n",
    "        \n",
    "        att_loss = self.calculate_loss(self.nnet.att_nnet, att_pis, att_vs, att_boards)\n",
    "            \n",
    "        att_optimizer = optim.Adam(self.nnet.att_nnet.parameters(), lr = lr)\n",
    "        self.nnet.att_nnet.train()\n",
    "\n",
    "        att_optimizer.zero_grad()\n",
    "        att_loss.backward()\n",
    "        att_optimizer.step()\n",
    "        self.nnet.att_nnet.eval()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def_boards, def_pis, def_vs = list(zip(*def_data))\n",
    "        \n",
    "        def_boards = np.array(list(def_boards))\n",
    "        def_pis = torch.FloatTensor(np.array(list(def_pis)))\n",
    "        def_vs = torch.FloatTensor(np.array(list(def_vs)))\n",
    "        \n",
    "        def_loss = self.calculate_loss(self.nnet.def_nnet, def_pis, def_vs, def_boards)\n",
    "            \n",
    "        def_optimizer = optim.Adam(self.nnet.def_nnet.parameters(), lr = lr)\n",
    "        self.nnet.def_nnet.train()\n",
    "\n",
    "        def_optimizer.zero_grad()\n",
    "        def_loss.backward()\n",
    "        def_optimizer.step()\n",
    "        self.nnet.def_nnet.eval()\n",
    "\n",
    "        return time.time()-start\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_es(self):\n",
    "        start = time.time()\n",
    "        att_data = []\n",
    "        def_data = []\n",
    "        for k in range(self.args.numEps):\n",
    "            att_boards, att_pis, def_boards, def_pis, r = self.execute_episode()\n",
    "            att_data += [(pq.get_attack_vector(board), self.att_pi_clip(pi), r) for (board, pi) in zip(att_boards, att_pis)]\n",
    "            def_data += [(pq.get_defend_vector(board), self.def_pi_clip(pi), r) for (board, pi) in zip(def_boards, def_pis)]\n",
    "            \n",
    "        board, player = self.g.getInitBoard()\n",
    "        att_boards, att_pis, att_vs = list(zip(*att_data))\n",
    "        \n",
    "        att_boards = np.array(list(att_boards))\n",
    "        att_pis = torch.FloatTensor(np.array(list(att_pis)))\n",
    "        att_vs = torch.FloatTensor(np.array(list(att_vs)))\n",
    "        \n",
    "        sample_models = [self.add_noise(self.nnet, std = self.sigma) for k in range(self.args.batch_size)]\n",
    "        att_models = [self.add_noise]\n",
    "        att_loss = self.calculate_loss(self.nnet.att_nnet, att_pis, att_vs, att_boards)\n",
    "            \n",
    "        att_optimizer = optim.Adam(self.nnet.att_nnet.parameters(), lr = lr)\n",
    "        self.nnet.att_nnet.train()\n",
    "\n",
    "        att_optimizer.zero_grad()\n",
    "        att_loss.backward()\n",
    "        att_optimizer.step()\n",
    "        self.nnet.att_nnet.eval()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def_boards, def_pis, def_vs = list(zip(*def_data))\n",
    "        \n",
    "        def_boards = np.array(list(def_boards))\n",
    "        def_pis = torch.FloatTensor(np.array(list(def_pis)))\n",
    "        def_vs = torch.FloatTensor(np.array(list(def_vs)))\n",
    "        \n",
    "        def_loss = self.calculate_loss(self.nnet.def_nnet, def_pis, def_vs, def_boards)\n",
    "            \n",
    "        def_optimizer = optim.Adam(self.nnet.def_nnet.parameters(), lr = lr)\n",
    "        self.nnet.def_nnet.train()\n",
    "\n",
    "        def_optimizer.zero_grad()\n",
    "        def_loss.backward()\n",
    "        def_optimizer.step()\n",
    "        self.nnet.def_nnet.eval()\n",
    "\n",
    "        return time.time()-start\n",
    "    \n",
    "\n",
    "    \n",
    "    def calculate_loss(self, model, target_pis, target_vs, boards):\n",
    "        out_pi, out_v = model.forward(boards)\n",
    "        l_pi = self.nnet.loss_pi(target_pis, out_pi)\n",
    "        log_l_pi = self.nnet.log_loss_pi(target_pis, out_pi)\n",
    "        l_v = self.nnet.loss_v(target_vs, out_v)\n",
    "        return l_pi + l_v\n",
    "    \n",
    "    def train_es(self):\n",
    "        start = time.time()\n",
    "        att_data = []\n",
    "        def_data = []\n",
    "        for k in range(self.args.numEps):\n",
    "            att_boards, att_pis, def_boards, def_pis, r = self.execute_episode()\n",
    "            att_data += [(pq.get_attack_vector(board), self.att_pi_clip(pi), r) for (board, pi) in zip(att_boards, att_pis)]\n",
    "            def_data += [(pq.get_defend_vector(board), self.def_pi_clip(pi), r) for (board, pi) in zip(def_boards, def_pis)]\n",
    "            \n",
    "            \n",
    "        att_boards, att_pis, att_vs = list(zip(*att_data))\n",
    "        \n",
    "        att_boards = np.array(list(att_boards))\n",
    "        att_pis = torch.FloatTensor(np.array(list(att_pis)))\n",
    "        att_vs = torch.FloatTensor(np.array(list(att_vs)))\n",
    "        \n",
    "        sample_models = [add_noise(self.nnet.att_nnet, std = self.sigma.att_nnet) for k in range(self.args.batch_size)]\n",
    "        att_models = [add_noise(self.nnet.att_nnet, std = self.sigma.att_nnet) for k in range(self.args.batch_size)]\n",
    "        att_losses = [self.calculate_loss(model, att_pis, att_vs, att_boards) for model in att_models]\n",
    "            \n",
    "        weights = log_weights(self.args.elite_size)\n",
    "        \n",
    "\n",
    "        d = {att_losses[k]: att_models[k] for k in range(self.args.batch_size)}\n",
    "        sort_keys = sorted(d)\n",
    "        elites = [d[key] for key in sort_keys[:self.args.elite_size]]\n",
    "        self.sigma.att_nnet = weighted_std(elites, weights, self.nnet.att_nnet, noise = 0.0001)\n",
    "        self.nnet.att_nnet = weighted_sum(elites, weights)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        def_boards, def_pis, def_vs = list(zip(*def_data))\n",
    "        \n",
    "        def_boards = np.array(list(def_boards))\n",
    "        def_pis = torch.FloatTensor(np.array(list(def_pis)))\n",
    "        def_vs = torch.FloatTensor(np.array(list(def_vs)))\n",
    "        \n",
    "        sample_models = [add_noise(self.nnet.def_nnet, std = self.sigma.def_nnet) for k in range(self.args.batch_size)]\n",
    "        def_models = [add_noise(self.nnet.def_nnet, std = self.sigma.def_nnet) for k in range(self.args.batch_size)]\n",
    "        def_losses = [self.calculate_loss(model, def_pis, def_vs, def_boards) for model in def_models]\n",
    "            \n",
    "        weights = log_weights(self.args.elite_size)\n",
    "        \n",
    "\n",
    "        d = {def_losses[k]: def_models[k] for k in range(self.args.batch_size)}\n",
    "        sort_keys = sorted(d)\n",
    "        elites = [d[key] for key in sort_keys[:self.args.elite_size]]\n",
    "        self.sigma.def_nnet = weighted_std(elites, weights, self.nnet.def_nnet, noise = 0.0001)\n",
    "        self.nnet.def_nnet = weighted_sum(elites, weights)\n",
    "        gc.collect()\n",
    "        \n",
    "        return time.time()-start\n",
    "        \n",
    "    \n",
    "AZ = AlphaZero(pq, nnet, mcts, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2c\n",
      "29, 34, 34, 27, 28, 24, [29, 34, 34, 27, 28, 24]\n",
      "az_es\n",
      "34, 21, 21, 33, 23, 36, [34, 21, 21, 33, 23, 36]\n",
      "az_grad\n",
      "35, 27, 22, 39, 27, 31, [35, 27, 22, 39, 27, 31]\n",
      "cem\n",
      "31, 32, 31, 43, 27, 45, [31, 32, 31, 43, 27, 45]\n",
      "oneone\n",
      "25, 30, 29, 35, 25, 32, [25, 30, 29, 35, 25, 32]\n",
      "random\n",
      "39, 29, 34, 20, 30, 38, [39, 29, 34, 20, 30, 38]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "wins = []\n",
    "count = 0\n",
    "np.random.seed(1)\n",
    "random.seed(0)\n",
    "for attacker in attackers:\n",
    "    print(names[count])\n",
    "    count += 1\n",
    "    att_wins = []\n",
    "    for defender in defenders:\n",
    "        \n",
    "        nnet = nn(pq)\n",
    "        \n",
    "        if type(attacker) != type(RandomAttackAgent(pq)):\n",
    "            nnet.att_nnet = attacker\n",
    "            \n",
    "        if type(defender) != type(RandomDefenseAgent(pq)):\n",
    "            nnet.def_nnet = defender\n",
    "        \n",
    "        mcts = MCTS(pq, nnet, args)\n",
    "        AZ = AlphaZero(pq, nnet, mcts, args)\n",
    "        win_count = 0\n",
    "        for k in range(100):\n",
    "            if AZ.execute_episode(render = False)[-1] == 1:\n",
    "                win_count += 1\n",
    "        print(win_count, end = ', ')\n",
    "        gc.collect()\n",
    "        att_wins.append(win_count)\n",
    "    wins.append(att_wins)\n",
    "    print(att_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& 29 & 34 & 34 & 27 & 28 & 24 & 29.3 \\\\\n",
      "& 34 & 21 & 21 & 33 & 23 & 36 & 28.0 \\\\\n",
      "& 35 & 27 & 22 & 39 & 27 & 31 & 30.2 \\\\\n",
      "& 31 & 32 & 31 & 43 & 27 & 45 & 34.8 \\\\\n",
      "& 25 & 30 & 29 & 35 & 25 & 32 & 29.3 \\\\\n",
      "& 39 & 29 & 34 & 20 & 30 & 38 & 31.7 \\\\\n"
     ]
    }
   ],
   "source": [
    "for w in wins:\n",
    "    for val in w:\n",
    "        print('& {} '.format(val), end = '')\n",
    "        \n",
    "    print('& {} '.format(np.round(np.mean(w), 1)), end = '')\n",
    "    print('\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump( wins, open( \"wins.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
