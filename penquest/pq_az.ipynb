{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from es_utils import *\n",
    "import math\n",
    "import numpy as np\n",
    "from pq_net import NNetWrapper as nn\n",
    "from pq_net import PQAC\n",
    "from penquest import *\n",
    "from utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 10,\n",
    "    'numEps': 20,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,        #\n",
    "    'updateThreshold': 0.5,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,    # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 10,          # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,         # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'batch_size' : 20,\n",
    "    'elite_size' : 10,\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "})\n",
    "\n",
    "EPS = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"\n",
    "    This class handles the MCTS tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game, nnet, args):\n",
    "        self.game = game\n",
    "        self.nnet = nnet\n",
    "        self.args = args\n",
    "        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n",
    "        self.Nsa = {}  # stores #times edge s,a was visited\n",
    "        self.Ns = {}  # stores #times board s was visited\n",
    "        self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "\n",
    "        self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "        self.Vs = {}  # stores game.getValidMoves for board s\n",
    "\n",
    "    def getActionProb(self, canonicalBoard, temp=1):\n",
    "        \"\"\"\n",
    "        This function performs numMCTSSims simulations of MCTS starting from\n",
    "        canonicalBoard.\n",
    "        Returns:\n",
    "            probs: a policy vector where the probability of the ith action is\n",
    "                   proportional to Nsa[(s,a)]**(1./temp)\n",
    "        \"\"\"\n",
    "        for i in range(self.args.numMCTSSims):\n",
    "            self.search(canonicalBoard)\n",
    "\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "        if temp == 0:\n",
    "            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n",
    "            bestA = np.random.choice(bestAs)\n",
    "            probs = [0] * len(counts)\n",
    "            probs[bestA] = 1\n",
    "            return probs\n",
    "\n",
    "        counts = [x ** (1. / temp) for x in counts]\n",
    "        counts_sum = float(sum(counts))\n",
    "        probs = [x / counts_sum for x in counts]\n",
    "        return probs\n",
    "    \n",
    "        \n",
    "\n",
    "    def search(self, canonicalBoard):\n",
    "        \"\"\"\n",
    "        This function performs one iteration of MCTS. It is recursively called\n",
    "        till a leaf node is found. The action chosen at each node is one that\n",
    "        has the maximum upper confidence bound as in the paper.\n",
    "        Once a leaf node is found, the neural network is called to return an\n",
    "        initial policy P and a value v for the state. This value is propagated\n",
    "        up the search path. In case the leaf node is a terminal state, the\n",
    "        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "        updated.\n",
    "        NOTE: the return values are the negative of the value of the current\n",
    "        state. This is done since v is in [-1,1] and if v is the value of a\n",
    "        state for the current player, then its value is -v for the other player.\n",
    "        Returns:\n",
    "            v: the negative of the value of the current canonicalBoard\n",
    "        \"\"\"\n",
    "\n",
    "        board, player = canonicalBoard\n",
    "        s = self.game.stringRepresentation(canonicalBoard)\n",
    "        \n",
    "        if s not in self.Es:\n",
    "            self.Es[s] = self.game.getGameEnded(board, 1)\n",
    "        if self.Es[s] != 0:\n",
    "            # terminal node\n",
    "            return -self.Es[s] * player\n",
    "\n",
    "        if s not in self.Ps:\n",
    "            # leaf node\n",
    "            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "            valids = self.game.getValidMoves(*canonicalBoard)\n",
    "            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "            sum_Ps_s = np.sum(self.Ps[s])\n",
    "            if sum_Ps_s > 0:\n",
    "                self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "            else:\n",
    "                # if all valid moves were masked make all valid moves equally probable\n",
    "\n",
    "                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n",
    "                print(\"All valid moves were masked, doing a workaround.\")\n",
    "                self.Ps[s] = self.Ps[s] + valids\n",
    "                self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "            self.Vs[s] = valids\n",
    "            self.Ns[s] = 0\n",
    "            return v\n",
    "\n",
    "        valids = self.Vs[s]\n",
    "        cur_best = -float('inf')\n",
    "        best_act = -1\n",
    "\n",
    "        # pick the action with the highest upper confidence bound\n",
    "        for a in range(self.game.getActionSize()):\n",
    "            if valids[a]:\n",
    "                if (s, a) in self.Qsa:\n",
    "                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n",
    "                            1 + self.Nsa[(s, a)])\n",
    "                else:\n",
    "                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n",
    "\n",
    "                if u > cur_best:\n",
    "                    cur_best = u\n",
    "                    best_act = a\n",
    "\n",
    "        a = best_act\n",
    "        next_s, next_player = self.game.getNextState(board, player, a)\n",
    "        next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "        v = self.search(next_s)\n",
    "\n",
    "        if (s, a) in self.Qsa:\n",
    "            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "            self.Nsa[(s, a)] += 1\n",
    "\n",
    "        else:\n",
    "            self.Qsa[(s, a)] = v\n",
    "            self.Nsa[(s, a)] = 1\n",
    "\n",
    "        self.Ns[s] += 1\n",
    "        return -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weights = [[1, 1, 0],\n",
    "         [1, 1, 1],\n",
    "         [0, 1, 1]]\n",
    "\n",
    "m_atts = [[1, 0, 0.5, 0, 0],\n",
    "          [0, 0, 0.5, 0, 0],\n",
    "          [0, 1, 0.5, 0, 0]]\n",
    "\n",
    "p1_atts = [1, 1, 1, 20, 1]\n",
    "p2_atts = [1, 1, 1, 20, 1]\n",
    "\n",
    "pq = PenQuest(weights, m_atts, p1_atts, p2_atts)\n",
    "board, player = pq.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nnet = nn(pq)\n",
    "mcts = MCTS(pq, nnet, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<penquest.Player object at 0xa37f59c88>, <penquest.Player object at 0xa37f59c18>, [<penquest.Machine object at 0xa37f59320>, <penquest.Machine object at 0xa37f594a8>, <penquest.Machine object at 0xa37f593c8>]), 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.13479015, 0.03250465, 0.8327052 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ]),\n",
       " array(0.40851513, dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canon = pq.getCanonicalForm(board, 1)\n",
    "print(canon)\n",
    "\n",
    "nnet.predict(canon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8888888888888888, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts.getActionProb((board, player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "class AlphaZero():\n",
    "    def __init__(self, game, nnet, mcts, args):\n",
    "        self.g = game\n",
    "        self.args = args\n",
    "        self.nnet = nnet\n",
    "        self.mcts = mcts\n",
    "        self.sigma = copy.copy(nnet)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in self.sigma.att_nnet.parameters():\n",
    "                param.divide_(10)\n",
    "            for param in self.sigma.def_nnet.parameters():\n",
    "                param.divide_(10)\n",
    "        \n",
    "    def execute_episode(self, render = False):\n",
    "        board, player = self.g.getInitBoard()\n",
    "        trainExamples = []\n",
    "        att_boards = []\n",
    "        att_pis = []\n",
    "        \n",
    "        def_boards = []\n",
    "        def_pis = []\n",
    "        \n",
    "        total_moves = 0\n",
    "        while True:\n",
    "            canonicalBoard = self.g.getCanonicalForm(board, player)\n",
    "            temp = 1 if total_moves < self.args.tempThreshold else 0\n",
    "            pi = self.mcts.getActionProb(canonicalBoard, temp = temp)\n",
    "            a = np.random.choice(len(pi), p = pi)\n",
    "            \n",
    "            if player == 1:\n",
    "                att_boards.append(board)\n",
    "                att_pis.append(pi)\n",
    "                \n",
    "            elif player == -1:\n",
    "                def_boards.append(board)\n",
    "                def_pis.append(pi)\n",
    "                \n",
    "            else:\n",
    "                print(\"bruhhhh\")\n",
    "\n",
    "                \n",
    "            board, player = self.g.getNextState(board, player, a) \n",
    "            total_moves +=1\n",
    "            \n",
    "            if render:\n",
    "                print('Probs: {}\\nAction: {}\\n'.format(pi, a))\n",
    "                \n",
    "            r = self.g.getGameEnded(board, player)\n",
    "            if r != 0:\n",
    "                return att_boards, att_pis, def_boards, def_pis, r\n",
    "            \n",
    "    def att_pi_clip(self, pi):\n",
    "        return pi[:self.g.size]\n",
    "    \n",
    "    def def_pi_clip(self, pi):\n",
    "        return pi[self.g.size:]\n",
    "    \n",
    "    def train_grad(self):\n",
    "        start = time.time()\n",
    "        att_data = []\n",
    "        def_data = []\n",
    "        for k in range(self.args.numEps):\n",
    "            att_boards, att_pis, def_boards, def_pis, r = self.execute_episode()\n",
    "            att_data += [(pq.get_attack_vector(board), self.att_pi_clip(pi), r) for (board, pi) in zip(att_boards, att_pis)]\n",
    "            def_data += [(pq.get_defend_vector(board), self.def_pi_clip(pi), r) for (board, pi) in zip(def_boards, def_pis)]\n",
    "            \n",
    "    \n",
    "        att_boards, att_pis, att_vs = list(zip(*att_data))\n",
    "        \n",
    "        att_boards = np.array(list(att_boards))\n",
    "        att_pis = torch.FloatTensor(np.array(list(att_pis)))\n",
    "        att_vs = torch.FloatTensor(np.array(list(att_vs)))\n",
    "        \n",
    "        att_loss = self.calculate_loss(self.nnet.att_nnet, att_pis, att_vs, att_boards)\n",
    "            \n",
    "        att_optimizer = optim.Adam(self.nnet.att_nnet.parameters(), lr = lr)\n",
    "        self.nnet.att_nnet.train()\n",
    "\n",
    "        att_optimizer.zero_grad()\n",
    "        att_loss.backward()\n",
    "        att_optimizer.step()\n",
    "        self.nnet.att_nnet.eval()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        def_boards, def_pis, def_vs = list(zip(*def_data))\n",
    "        \n",
    "        def_boards = np.array(list(def_boards))\n",
    "        def_pis = torch.FloatTensor(np.array(list(def_pis)))\n",
    "        def_vs = torch.FloatTensor(np.array(list(def_vs)))\n",
    "        \n",
    "        def_loss = self.calculate_loss(self.nnet.def_nnet, def_pis, def_vs, def_boards)\n",
    "            \n",
    "        def_optimizer = optim.Adam(self.nnet.def_nnet.parameters(), lr = lr)\n",
    "        self.nnet.def_nnet.train()\n",
    "\n",
    "        def_optimizer.zero_grad()\n",
    "        def_loss.backward()\n",
    "        def_optimizer.step()\n",
    "        self.nnet.def_nnet.eval()\n",
    "\n",
    "        return time.time()-start\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_es(self):\n",
    "        start = time.time()\n",
    "        att_data = []\n",
    "        def_data = []\n",
    "        for k in range(self.args.numEps):\n",
    "            att_boards, att_pis, def_boards, def_pis, r = self.execute_episode()\n",
    "            att_data += [(pq.get_attack_vector(board), self.att_pi_clip(pi), r) for (board, pi) in zip(att_boards, att_pis)]\n",
    "            def_data += [(pq.get_defend_vector(board), self.def_pi_clip(pi), r) for (board, pi) in zip(def_boards, def_pis)]\n",
    "            \n",
    "            \n",
    "        att_boards, att_pis, att_vs = list(zip(*att_data))\n",
    "        \n",
    "        att_boards = np.array(list(att_boards))\n",
    "        att_pis = torch.FloatTensor(np.array(list(att_pis)))\n",
    "        att_vs = torch.FloatTensor(np.array(list(att_vs)))\n",
    "        \n",
    "        sample_models = [add_noise(self.nnet.att_nnet, std = self.sigma.att_nnet) for k in range(self.args.batch_size)]\n",
    "        att_models = [add_noise(self.nnet.att_nnet, std = self.sigma.att_nnet) for k in range(self.args.batch_size)]\n",
    "        att_losses = [self.calculate_loss(model, att_pis, att_vs, att_boards) for model in att_models]\n",
    "            \n",
    "        weights = log_weights(self.args.elite_size)\n",
    "        \n",
    "\n",
    "        d = {att_losses[k]: att_models[k] for k in range(self.args.batch_size)}\n",
    "        sort_keys = sorted(d)\n",
    "        elites = [d[key] for key in sort_keys[:self.args.elite_size]]\n",
    "        self.sigma.att_nnet = weighted_std(elites, weights, self.nnet.att_nnet, noise = 0.0001)\n",
    "        self.nnet.att_nnet = weighted_sum(elites, weights)\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        def_boards, def_pis, def_vs = list(zip(*def_data))\n",
    "        \n",
    "        def_boards = np.array(list(def_boards))\n",
    "        def_pis = torch.FloatTensor(np.array(list(def_pis)))\n",
    "        def_vs = torch.FloatTensor(np.array(list(def_vs)))\n",
    "        \n",
    "        sample_models = [add_noise(self.nnet.def_nnet, std = self.sigma.def_nnet) for k in range(self.args.batch_size)]\n",
    "        def_models = [add_noise(self.nnet.def_nnet, std = self.sigma.def_nnet) for k in range(self.args.batch_size)]\n",
    "        def_losses = [self.calculate_loss(model, def_pis, def_vs, def_boards) for model in def_models]\n",
    "            \n",
    "        weights = log_weights(self.args.elite_size)\n",
    "        \n",
    "\n",
    "        d = {def_losses[k]: def_models[k] for k in range(self.args.batch_size)}\n",
    "        sort_keys = sorted(d)\n",
    "        elites = [d[key] for key in sort_keys[:self.args.elite_size]]\n",
    "        self.sigma.def_nnet = weighted_std(elites, weights, self.nnet.def_nnet, noise = 0.0001)\n",
    "        self.nnet.def_nnet = weighted_sum(elites, weights)\n",
    "        gc.collect()\n",
    "        \n",
    "        return time.time()-start\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    def calculate_loss(self, model, target_pis, target_vs, boards):\n",
    "        out_pi, out_v = model.forward(boards)\n",
    "        l_pi = self.nnet.loss_pi(target_pis, out_pi)\n",
    "        log_l_pi = self.nnet.log_loss_pi(target_pis, out_pi)\n",
    "        l_v = self.nnet.loss_v(target_vs, out_v)\n",
    "        return l_pi + l_v\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZ = AlphaZero(pq, nnet, mcts, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "6\n",
      "322.3623170852661\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(1)\n",
    "\n",
    "t_max = 300\n",
    "\n",
    "start = time.time()\n",
    "iters = 0\n",
    "while time.time() - start < t_max:\n",
    "    iters += 1\n",
    "    AZ.train_es()\n",
    "    print(iters)\n",
    "    \n",
    "print(iters)\n",
    "print(time.time() - start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AZ.execute_episode(render = False)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAttackAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * self.g.size), 0\n",
    "    \n",
    "class RandomDefenseAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * (self.g.size*2)), 0\n",
    "\n",
    "\n",
    "def play_game(agent_1, agent_2, render = False, temp = 1):\n",
    "    board, player = pq.getInitBoard()\n",
    "    boards = []\n",
    "    players = []\n",
    "    pis = []\n",
    "    values = []\n",
    "    while pq.getGameEnded(board, player) == 0:\n",
    "        valids = pq.getValidMoves(board, player)\n",
    "        if player == 1:\n",
    "            vec = pq.get_attack_vector(board)\n",
    "            probs, value = agent_1.forward(vec)\n",
    "            valids = valids[:pq.size]\n",
    "        elif player == -1:\n",
    "            vec = pq.get_defend_vector(board)\n",
    "            probs, value = agent_2.forward(vec)\n",
    "            valids = valids[pq.size:]\n",
    "        else:\n",
    "            print('bruh')\n",
    "            \n",
    "        probs = probs.detach().numpy()\n",
    "\n",
    "        \n",
    "        boards.append(board)\n",
    "        players.append(player)\n",
    "        pis.append(probs)\n",
    "        values.append(value)\n",
    "        \n",
    "        if temp != 0:\n",
    "            probs = np.power(probs, temp)\n",
    "        probs = np.array(probs) * np.array(valids)\n",
    "        probs = np.squeeze(probs)\n",
    "        if sum(probs) == 0:\n",
    "            print('bruh')\n",
    "        probs = probs / np.sum(probs)\n",
    "        if temp == 0:\n",
    "            action = np.argmax(probs)\n",
    "        else:\n",
    "            action = np.random.choice(len(probs), p = probs)\n",
    "        if player == -1:\n",
    "            action += pq.size\n",
    "        \n",
    "        board, player = pq.getNextState(board, player, action, render = render)\n",
    "        if render:\n",
    "            pq.render(board, player)\n",
    "          \n",
    "    \n",
    "    return pq.getGameEnded(board, player)\n",
    "\n",
    "\n",
    "def attacker_vs_defender(attacker, defender, episode_count, temp = 1):\n",
    "    wins = 0\n",
    "    for k in range(episode_count):\n",
    "        winner = play_game(attacker, defender, temp = temp)\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "            \n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(attacker_vs_defender(AZ.nnet.att_nnet, RandomDefenseAgent(pq), 100))\n",
    "print(attacker_vs_defender(RandomAttackAgent(pq), AZ.nnet.def_nnet, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.4721, 0.3133, 0.2145]], grad_fn=<SoftmaxBackward>), tensor([[-0.4764]], grad_fn=<AddmmBackward>))\n",
      "(tensor([[0.1244, 0.3195, 0.1203, 0.1508, 0.1586, 0.1265]],\n",
      "       grad_fn=<SoftmaxBackward>), tensor([[-0.7565]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "board, player = pq.getInitBoard()\n",
    "\n",
    "print(AZ.nnet.att_nnet.forward(pq.get_attack_vector(board)))\n",
    "print(AZ.nnet.def_nnet.forward(pq.get_defend_vector(board)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Directory exists! \n"
     ]
    }
   ],
   "source": [
    "nnet.save_checkpoint(att_filename='az_es_att_checkpoint.pth.tar', def_filename='az_es_def_checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
