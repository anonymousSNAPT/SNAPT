{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from pq_net import PQAC\n",
    "from penquest import *\n",
    "sys.path.append('../')\n",
    "from es_utils import *\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAttackAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * self.g.size), 0\n",
    "    \n",
    "class RandomDefenseAgent():\n",
    "    def __init__(self, g):\n",
    "        self.g = g\n",
    "        \n",
    "    def forward(self, board):\n",
    "        return torch.FloatTensor([1] * (self.g.size*2)), 0\n",
    "\n",
    "def play_game(agent_1, agent_2, render = False, temp = 1):\n",
    "    board, player = pq.getInitBoard()\n",
    "    boards = []\n",
    "    players = []\n",
    "    pis = []\n",
    "    values = []\n",
    "    while pq.getGameEnded(board, player) == 0:\n",
    "        valids = pq.getValidMoves(board, player)\n",
    "        if player == 1:\n",
    "            vec = pq.get_attack_vector(board)\n",
    "            probs, value = agent_1.forward(vec)\n",
    "            valids = valids[:pq.size]\n",
    "        elif player == -1:\n",
    "            vec = pq.get_defend_vector(board)\n",
    "            probs, value = agent_2.forward(vec)\n",
    "            valids = valids[pq.size:]\n",
    "        else:\n",
    "            print('bruh')\n",
    "            \n",
    "        probs = probs.detach().numpy()\n",
    "\n",
    "        \n",
    "        boards.append(board)\n",
    "        players.append(player)\n",
    "        pis.append(probs)\n",
    "        values.append(value)\n",
    "        \n",
    "        if temp != 0:\n",
    "            probs = np.power(probs, temp)\n",
    "        probs = np.array(probs) * np.array(valids)\n",
    "        probs = np.squeeze(probs)\n",
    "        if sum(probs) == 0:\n",
    "            print('bruh')\n",
    "        probs = probs / np.sum(probs)\n",
    "        if temp == 0:\n",
    "            action = np.argmax(probs)\n",
    "        else:\n",
    "            action = np.random.choice(len(probs), p = probs)\n",
    "        if player == -1:\n",
    "            action += pq.size\n",
    "        \n",
    "        board, player = pq.getNextState(board, player, action, render = render)\n",
    "        if render:\n",
    "            pq.render(board, player)\n",
    "          \n",
    "    \n",
    "    return pq.getGameEnded(board, player), [(b, pl, pi, v, pl * pq.getGameEnded(board, player)) for (b, pl, pi, v) in zip(boards, players, pis, values)]\n",
    "        \n",
    "\n",
    "    \n",
    "def attacker_vs_random(attacker, episode_count, temp = 1):\n",
    "    wins = 0\n",
    "    for k in range(episode_count):\n",
    "        winner = play_game(attacker, RandomDefenseAgent(pq), temp = temp)[0]\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "            \n",
    "    return wins/episode_count\n",
    "\n",
    "def defender_vs_random(defender, episode_count, temp = 1):\n",
    "    wins = 0\n",
    "    for k in range(episode_count):\n",
    "        winner = play_game(RandomAttackAgent(pq), defender, temp = temp)[0]\n",
    "        if winner == -1:\n",
    "            wins += 1\n",
    "            \n",
    "    return wins/episode_count\n",
    "\n",
    "def attacker_vs_defender(attacker, defender, episode_count, temp = 1):\n",
    "    wins = 0\n",
    "    for k in range(episode_count):\n",
    "        winner = play_game(attacker, defender, temp = temp)[0]\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "            \n",
    "    return wins\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n"
     ]
    }
   ],
   "source": [
    "weights = [[1, 1, 0],\n",
    "         [1, 1, 1],\n",
    "         [0, 1, 1]]\n",
    "\n",
    "m_atts = [[1, 0, 0.5, 0, 0],\n",
    "          [0, 0, 0.5, 0, 0],\n",
    "          [0, 1, 0.5, 0, 0]]\n",
    "\n",
    "p1_atts = [1, 1, 1, 20, 1]\n",
    "p2_atts = [1, 1, 1, 20, 1]\n",
    "\n",
    "pq = PenQuest(weights, m_atts, p1_atts, p2_atts)\n",
    "board, player = pq.getInitBoard()\n",
    "wins = 0\n",
    "games = 10\n",
    "for j in range(games): \n",
    "    winner = play_game(RandomAttackAgent(pq), RandomDefenseAgent(pq))[0]\n",
    "    if winner == 1:\n",
    "        wins += 1\n",
    "\n",
    "print(wins/games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play_game(RandomAttackAgent(pq), RandomDefenseAgent(pq), render = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_inputs = len(pq.get_attack_vector(board))\n",
    "defender_inputs = len(pq.get_defend_vector(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker = PQAC(attacker_inputs, pq.size, 64)\n",
    "defender = PQAC(defender_inputs, 2 * pq.size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_pq(g, iters = 100, batch_size = 20, elite_size = 10, episode_count = 10, weight_type = 'even', temp = 1):\n",
    "    att_mu = PQAC(attacker_inputs, pq.size, 64)\n",
    "    def_mu = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "    \n",
    "    att_sigma = copy.deepcopy(att_mu)\n",
    "    def_sigma = copy.deepcopy(def_mu)\n",
    "    \n",
    "    att_rewards = []\n",
    "    def_rewards = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in att_sigma.parameters():\n",
    "            param.divide_(10)\n",
    "        for param in def_sigma.parameters():\n",
    "            param.divide_(10)\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "            \n",
    "        att_models = [add_noise(att_mu, std = att_sigma) for k in range(batch_size)]\n",
    "        def_models = [add_noise(def_mu, std = def_sigma) for k in range(batch_size)]\n",
    "        \n",
    "        att_values = [attacker_vs_defender(att_model, def_mu, episode_count, temp = temp) for att_model in att_models]\n",
    "        def_values = [attacker_vs_defender(att_mu, def_model, episode_count, temp = temp) for def_model in def_models]\n",
    "        \n",
    "        att_dict = {(att_values[k] + (k/(10**5))): att_models[k] for k in range(batch_size)}\n",
    "        def_dict = {(def_values[k] + (k/(10**5))): def_models[k] for k in range(batch_size)}\n",
    "        \n",
    "        att_keys = list(reversed(sorted(att_dict)))\n",
    "        def_keys = list(sorted(def_dict))\n",
    "        \n",
    "        att_elites = [att_dict[key] for key in att_keys[:elite_size]]\n",
    "        def_elites = [def_dict[key] for key in def_keys[:elite_size]]\n",
    "\n",
    "        if weight_type == 'even':\n",
    "            weights = [1/elite_size for k in range(elite_size)]\n",
    "        elif weight_type == 'log':\n",
    "            weights = log_weights(elite_size)\n",
    "        else:\n",
    "            print('bruh')\n",
    "            return None\n",
    "        \n",
    "        att_sigma = weighted_std(att_elites, weights, att_mu, noise = 0.000001)\n",
    "        att_mu = weighted_sum(att_elites, weights)\n",
    "        \n",
    "        def_sigma = weighted_std(def_elites, weights, def_mu, noise = 0.000001)\n",
    "        def_mu = weighted_sum(def_elites, weights)\n",
    "        \n",
    "        att_rewards.append(attacker_vs_random(att_mu, episode_count, temp = temp))\n",
    "        def_rewards.append(defender_vs_random(def_mu, episode_count, temp = temp))\n",
    "        \n",
    "        gc.collect()\n",
    "        if (i + 1)  % (iters// 20) == 0:\n",
    "            current = time.time()\n",
    "            print('Finished iteration {}. {} seconds elapsed.'.format(i+1, np.round(current-start, 2)))\n",
    "            print(att_rewards[-1], def_rewards[-1])\n",
    "            \n",
    "    return att_mu, att_rewards, def_mu, def_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneone_pq(g, iters = 100, episode_count = 10, temp = 1, t_max = 3600):\n",
    "    att_mu = PQAC(attacker_inputs, pq.size, 64)\n",
    "    def_mu = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "    \n",
    "    att_sigma = copy.deepcopy(att_mu)\n",
    "    def_sigma = copy.deepcopy(def_mu)\n",
    "    \n",
    "    att_rewards = []\n",
    "    def_rewards = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in att_sigma.parameters():\n",
    "            param.divide_(10)\n",
    "        for param in def_sigma.parameters():\n",
    "            param.divide_(10)\n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(iters):\n",
    "            \n",
    "        att_model = add_noise(att_mu, std = att_sigma)\n",
    "        def_model = add_noise(def_mu, std = def_sigma)\n",
    "        \n",
    "        default = attacker_vs_defender(att_mu, def_mu, episode_count, temp = temp)\n",
    "        att_value = attacker_vs_defender(att_model, def_mu, episode_count, temp = temp)\n",
    "        def_value = attacker_vs_defender(att_mu, def_model, episode_count, temp = temp)\n",
    "        \n",
    "        if att_value > default:\n",
    "            att_sigma = weighted_std([att_model], [1], att_mu, noise = 0.000001)\n",
    "            att_mu = att_model\n",
    "        \n",
    "        if def_value < default:\n",
    "            def_sigma = weighted_std([def_model], [1], def_mu, noise = 0.000001)\n",
    "            def_mu = def_model\n",
    "        \n",
    "        att_rewards.append(attacker_vs_random(att_mu, episode_count, temp = temp))\n",
    "        def_rewards.append(defender_vs_random(def_mu, episode_count, temp = temp))\n",
    "        \n",
    "        gc.collect()\n",
    "        current = time.time()\n",
    "        if current -start > t_max:\n",
    "            print('Finished in {} seconds and {} iterations'.format(np.round(current-start), i+1))\n",
    "            break\n",
    "        if (i + 1)  % (iters// 20) == 0:\n",
    "            print('Finished iteration {}. {} seconds elapsed.'.format(i+1, np.round(current-start, 2)))\n",
    "            print(att_rewards[-1], def_rewards[-1])\n",
    "            \n",
    "    return att_mu, att_rewards, def_mu, def_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_pq(g, iters = 100, t_max = 3600, temp = 1, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "    start =time.time()\n",
    "    learning_rate = 0.0001\n",
    "    att_ac = PQAC(attacker_inputs, pq.size, 64)\n",
    "    def_ac = PQAC(defender_inputs, 2 * pq.size, 64)\n",
    "    att_optimizer = optim.Adam(att_ac.parameters(), lr=learning_rate)\n",
    "    def_optimizer = optim.Adam(def_ac.parameters(), lr=learning_rate)\n",
    "    att_ac.eval()\n",
    "    def_ac.eval()\n",
    "    entropy_term = 0\n",
    "    \n",
    "    for i in range(iters):\n",
    "        gc.collect()\n",
    "        att_log_probs = []\n",
    "        att_values = []\n",
    "        \n",
    "        def_log_probs = []\n",
    "        def_values = []\n",
    "        \n",
    "        players = []\n",
    "\n",
    "        board, player = g.getInitBoard()\n",
    "        player = 1\n",
    "        attack_vec = g.get_attack_vector(board)\n",
    "        #print(att_ac.forward(attack_vec))\n",
    "        while True:\n",
    "            valids = pq.getValidMoves(board, player)\n",
    "            if player == 1:\n",
    "                vec = pq.get_attack_vector(board)\n",
    "                probs, value = att_ac(vec)\n",
    "                valids = valids[:pq.size]\n",
    "                att_log_prob = torch.log(probs.squeeze())\n",
    "                att_log_probs.append(att_log_prob)\n",
    "                att_values.append(value.squeeze())\n",
    "                \n",
    "            elif player == -1:\n",
    "                vec = pq.get_defend_vector(board)\n",
    "                probs, value = def_ac.forward(vec)\n",
    "                valids = valids[pq.size:]\n",
    "                def_log_prob = torch.log(probs.squeeze())\n",
    "                def_log_probs.append(def_log_prob)\n",
    "                def_values.append(value)\n",
    "                \n",
    "            probs = np.squeeze(probs.detach().numpy())\n",
    "            entropy = -np.sum(np.mean(probs) * np.log(probs))\n",
    "            entropy_term += entropy\n",
    "            \n",
    "            probs = np.array(probs) * np.array(valids)\n",
    "            if np.isnan(probs).any():\n",
    "                print('bruhh')\n",
    "                att_ac, def_ac\n",
    "            \n",
    "            if temp == 0:\n",
    "                action = np.argmax(probs)\n",
    "                \n",
    "            else:\n",
    "                probs = np.power(probs, temp)\n",
    "                probs = probs/np.sum(probs)\n",
    "                action = np.random.choice(len(probs), p = probs)\n",
    "            \n",
    "            \n",
    "            board, player = g.getNextState(board, player, action)\n",
    "            \n",
    "            r = g.getGameEnded(board, player)\n",
    "            \n",
    "            if r != 0:\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        att_Qvals = np.zeros_like(att_values)\n",
    "        att_Qvals += r\n",
    "        \n",
    "        def_Qvals = np.zeros_like(def_values)\n",
    "        def_Qvals -= r\n",
    "            \n",
    "        \n",
    "        #update actor critic\n",
    "        att_ac.train()\n",
    "        att_values = torch.stack(att_values)\n",
    "        att_Qvals = att_Qvals.astype(np.float32)\n",
    "        att_Qvals = torch.FloatTensor(att_Qvals)\n",
    "        att_log_probs = torch.stack(att_log_probs)\n",
    "        \n",
    "        att_advantage = att_Qvals - att_values\n",
    "        att_advantage = att_advantage.unsqueeze(-1)\n",
    "        att_actor_loss = (-att_log_probs * att_advantage).mean()\n",
    "        att_critic_loss = 0.5 * att_advantage.pow(2).mean()\n",
    "        att_loss = att_actor_loss + att_critic_loss + 0.0001 * entropy_term\n",
    "        #print(att_loss)\n",
    "        #print([((a.grad!=None), a.requires_grad) for a in list(att_ac.parameters())])\n",
    "        att_loss.backward()\n",
    "        #print([((a.grad!=None), a.requires_grad) for a in list(att_ac.parameters())])\n",
    "        att_optimizer.step()\n",
    "        att_optimizer.zero_grad()\n",
    "        #print([((a.grad!=None), a.requires_grad) for a in list(att_ac.parameters())])\n",
    "        att_ac.eval()\n",
    "        \n",
    "        #update actor critic\n",
    "        \n",
    "        def_ac.train()\n",
    "        def_values = torch.stack(def_values)\n",
    "        def_Qvals = def_Qvals.astype(np.float32)\n",
    "        def_Qvals = torch.FloatTensor(def_Qvals)\n",
    "        def_log_probs = torch.stack(def_log_probs)\n",
    "        \n",
    "        def_advantage = def_Qvals - def_values\n",
    "        def_advantage = def_advantage.unsqueeze(-1)\n",
    "        def_actor_loss = (-def_log_probs * def_advantage).mean()\n",
    "        def_critic_loss = 0.5 * def_advantage.pow(2).mean()\n",
    "        def_loss = def_actor_loss + def_critic_loss + 0.0001 * entropy_term\n",
    "        \n",
    "        \n",
    "        def_optimizer.zero_grad()\n",
    "        def_loss.backward()\n",
    "        def_optimizer.step()\n",
    "        def_ac.eval()\n",
    "        \n",
    "        \n",
    "        current = time.time()\n",
    "        if current -start > t_max:\n",
    "            print('Finished in {} seconds and {} iterations'.format(np.round(current-start), i+1))\n",
    "            break\n",
    "        \n",
    "        if (i + 1)  % max(1,(iters// 20)) == 0:\n",
    "            \n",
    "            print('Finished iteration {}. {} seconds elapsed.'.format(i+1, np.round(current-start, 2)))\n",
    "        \n",
    "        \n",
    "    return att_ac, def_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration 25. 2.05 seconds elapsed.\n",
      "Finished iteration 50. 3.91 seconds elapsed.\n",
      "Finished iteration 75. 5.77 seconds elapsed.\n",
      "Finished iteration 100. 7.63 seconds elapsed.\n",
      "Finished iteration 125. 9.87 seconds elapsed.\n",
      "Finished iteration 150. 12.26 seconds elapsed.\n",
      "Finished iteration 175. 14.31 seconds elapsed.\n",
      "Finished iteration 200. 16.34 seconds elapsed.\n",
      "Finished iteration 225. 18.4 seconds elapsed.\n",
      "Finished iteration 250. 20.39 seconds elapsed.\n",
      "Finished iteration 275. 22.73 seconds elapsed.\n",
      "Finished iteration 300. 24.61 seconds elapsed.\n",
      "Finished iteration 325. 26.48 seconds elapsed.\n",
      "Finished iteration 350. 28.36 seconds elapsed.\n",
      "Finished iteration 375. 30.93 seconds elapsed.\n",
      "Finished iteration 400. 32.87 seconds elapsed.\n",
      "Finished iteration 425. 34.77 seconds elapsed.\n",
      "Finished iteration 450. 36.63 seconds elapsed.\n",
      "Finished iteration 475. 38.57 seconds elapsed.\n",
      "Finished iteration 500. 40.64 seconds elapsed.\n",
      "0.8\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "att_ac, def_ac = a2c_pq(pq, iters = 500, t_max = 300, seed = 2)\n",
    "print(attacker_vs_random(att_ac, 100, temp = 2))\n",
    "print(attacker_vs_random(att_ac, 100, temp = 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0553, 0.4399, 0.5048]], grad_fn=<SoftmaxBackward>), tensor([[0.2995]], grad_fn=<AddmmBackward>))\n",
      "(tensor([[0.2149, 0.0100, 0.1744, 0.2972, 0.2702, 0.0334]],\n",
      "       grad_fn=<SoftmaxBackward>), tensor([[1.1593]], grad_fn=<AddmmBackward>))\n"
     ]
    }
   ],
   "source": [
    "attack_vec = pq.get_attack_vector(board)\n",
    "def_vec = pq.get_defend_vector(board)\n",
    "print(att_ac.forward(attack_vec))\n",
    "print(def_ac.forward(def_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62\n",
      "0.54\n",
      "0.29\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(attacker_vs_random(att_ac, 100, temp = 2))\n",
    "print(attacker_vs_random(att_ac, 100, temp = 1))\n",
    "\n",
    "print(defender_vs_random(def_ac, 100, temp = 2))\n",
    "print(defender_vs_random(def_ac, 100, temp = 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
